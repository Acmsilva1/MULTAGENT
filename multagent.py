import streamlit as st
from groq import Groq

# --- 1. CONFIGURA√á√ÉO DE SEGURAN√áA ---
# S√™nior avisando: Certifique-se que o nome no Secrets √© LLAMA_API_KEY
try:
    LLAMA_KEY = st.secrets["LLAMA_API_KEY"]
    client = Groq(api_key=LLAMA_KEY)
except Exception as e:
    st.error("üö® Erro nos Secrets! A chave 'LLAMA_API_KEY' n√£o foi encontrada.")
    st.stop()

# --- 2. PERSONALIDADE DO AGENTE (SYSTEM PROMPT) ---
# Aqui injetamos o sarcasmo e a expertise em TI/LGPD que voc√™ pediu
SYSTEM_PROMPT = """
Voc√™ √© o 'S√™nior √Åcido', um mentor de TI veterano.
- Personalidade: Sarc√°stico, assertivo e direto. Use analogias de TI (ex: comparar RAM com mesa de trabalho).
- Foco: IA, Dados e LGPD. 
- Governan√ßa: Se o usu√°rio enviar dados sens√≠veis, d√™ um alerta imediato.
- Estilo: Sem enrola√ß√£o. Se a d√∫vida for boba, seja ironicamente pedag√≥gico.
"""

# --- 3. CONFIGURA√á√ÉO DA P√ÅGINA (INTERFACE) ---
st.set_page_config(page_title="S√™nior √Åcido AI", page_icon="ü¶ô", layout="centered")
st.title("ü¶ô S√™nior √Åcido v2.0")
st.caption("Status: Llama 3.3 Online | Gemini: De castigo (Erro 404)")

# Inicializa o hist√≥rico se n√£o existir (Mem√≥ria de Sess√£o)
if "messages" not in st.session_state:
    st.session_state.messages = []

# Exibir mensagens anteriores (Persist√™ncia visual)
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- 4. O FLUXO DE CONVERSA ---
if prompt := st.chat_input("Diga l√°, futuro mestre dos dados..."):
    # Adiciona pergunta do usu√°rio ao hist√≥rico
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("Consultando meus neur√¥nios de sil√≠cio..."):
            try:
                # Tentativa com o modelo 70B (O c√©rebro grande)
                chat_completion = client.chat.completions.create(
                    messages=[
                        {"role": "system", "content": SYSTEM_PROMPT},
                        *st.session_state.messages # Envia todo o contexto
                    ],
                    model="llama-3.3-70b-versatile",
                    temperature=0.7,
                )
                resposta = chat_completion.choices[0].message.content
                
            except Exception as e:
                # Fallback: Se o grande falhar (cota/deprecia√ß√£o), tenta o r√°pido (8B)
                try:
                    chat_completion = client.chat.completions.create(
                        messages=[{"role": "system", "content": SYSTEM_PROMPT}, *st.session_state.messages],
                        model="llama-3.1-8b-instant",
                        temperature=0.7,
                    )
                    resposta = chat_completion.choices[0].message.content
                except Exception as e_final:
                    resposta = f"Deu tela azul aqui! Erro: {str(e_final)}"

            st.markdown(resposta)
            # Salva a resposta para manter o fio da meada
            st.session_state.messages.append({"role": "assistant", "content": resposta})

# --- 5. GOVERNAN√áA E LIMPEZA (SIDEBAR) ---
st.sidebar.header("Configura√ß√µes de Sess√£o")
if st.sidebar.button("üóëÔ∏è Limpar Conversa (LGPD)"):
    st.session_state.messages = []
    st.rerun()
