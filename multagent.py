import streamlit as st
from groq import Groq

# --- 1. CONFIGURA√á√ÉO DE SEGURAN√áA ---
try:
    # Como o Gemini est√° bugado, vamos focar apenas no Llama por enquanto
    LLAMA_KEY = st.secrets["LLAMA_API_KEY"]
    client = Groq(api_key=LLAMA_KEY)
except Exception as e:
    st.error("Erro nos Secrets! A chave LLAMA_API_KEY precisa estar configurada.")
    st.stop()

# --- 2. O SYSTEM PROMPT (A ALMA DO AGENTE) ---
# Aqui colocamos toda a expertise e o sarcasmo que voc√™ gosta
SYSTEM_PROMPT = """
Voc√™ √© o 'S√™nior √Åcido', um mentor de TI expert em IA, Dados e LGPD.
1. Personalidade: Sarc√°stico, assertivo e direto ao ponto.
2. M√©todo: Use analogias do dia a dia para explicar conceitos complexos.
3. Regra de Ouro: Se o usu√°rio falar sobre dados sens√≠veis, d√™ um alerta de seguran√ßa imediato.
4. Tom de voz: Trate o usu√°rio como um 'padawan' de TI que precisa de orienta√ß√£o real, sem enrola√ß√£o.
"""

# --- 3. PERSIST√äNCIA DE MEM√ìRIA (SESSION STATE) ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# --- 4. INTERFACE STREAMLIT ---
st.set_page_config(page_title="S√™nior √Åcido (Llama Edition)", page_icon="ü¶ô")
st.title("ü¶ô S√™nior √Åcido: O Agente Expert")
st.caption("Operando via Llama 3 (Groq) | Backup Gemini: Desativado por mau comportamento.")

# Exibir hist√≥rico de chat
for m in st.session_state.messages:
    with st.chat_message(m["role"]):
        st.markdown(m["content"])

# --- 5. O LOOP DE INTERA√á√ÉO ---
if prompt := st.chat_input("Mande sua d√∫vida t√©cnica..."):
    # Adiciona a pergunta do usu√°rio ao hist√≥rico
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        try:
            # Chamada direta para o Llama 3 - R√°pida e est√°vel
            chat_completion = client.chat.completions.create(
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT},
                    # Passamos o hist√≥rico para ele ter contexto sequencial
                    *st.session_state.messages 
                ],
                model="llama3-70b-8192", # O modelo mais inteligente do Llama no Groq
                temperature=0.7, # Para manter a criatividade no sarcasmo
            )
            
            resposta = chat_completion.choices[0].message.content
            st.markdown(resposta)
            
            # Salva a resposta do agente no hist√≥rico
            st.session_state.messages.append({"role": "assistant", "content": resposta})
            
        except Exception as e:
            st.error(f"At√© o Llama cansou! Erro: {str(e)}")

# --- 6. GOVERNAN√áA (BOT√ÉO DE LIMPEZA) ---
if st.sidebar.button("Limpar Sess√£o (LGPD)"):
    st.session_state.messages = []
    st.rerun()
