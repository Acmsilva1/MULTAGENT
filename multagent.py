import streamlit as st
import google.generativeai as genai
from groq import Groq # Biblioteca comum para rodar Llama 3

# --- 1. CONFIGURA√á√ÉO DE SEGURAN√áA ---
try:
    GEMINI_KEY = st.secrets["GEMINI_API_KEY"]
    LLAMA_KEY = st.secrets["LLAMA_API_KEY"]
    
    genai.configure(api_key=GEMINI_KEY)
    llama_client = Groq(api_key=LLAMA_KEY)
except KeyError as e:
    st.error(f"Falta a chave: {e}. Configure nos Secrets!")
    st.stop()

# --- 2. PERSONALIDADES (SYSTEM PROMPTS) ---
SYSTEM_GEMINI = "Voc√™ √© um Engenheiro de IA anal√≠tico. Forne√ßa respostas t√©cnicas e detalhadas sobre TI."
SYSTEM_LLAMA_REVISOR = """
Voc√™ √© o 'S√™nior √Åcido'. Sua fun√ß√£o √© revisar a resposta do outro agente.
1. Seja sarc√°stico e use analogias pr√°ticas.
2. Se o outro agente esqueceu algo de LGPD ou seguran√ßa, aponte o erro.
3. Se a resposta estiver boa, apenas a torne mais 'direta ao ponto' e engra√ßada.
"""

# --- 3. LOGICA MULTI-AGENTE ---
def fluxo_multi_agente(pergunta_usuario):
    # Passo 1: O Gemini gera a base t√©cnica
    model_gemini = genai.GenerativeModel('gemini-1.5-flash-latest')
    res_gemini = model_gemini.generate_content(f"{SYSTEM_GEMINI}\n\nPergunta: {pergunta_usuario}")
    texto_base = res_gemini.text

    # Passo 2: O Llama revisa (O 'Refinador')
    res_llama = llama_client.chat.completions.create(
        model="llama3-70b-8192", # Vers√£o potente do Llama
        messages=[
            {"role": "system", "content": SYSTEM_LLAMA_REVISOR},
            {"role": "user", "content": f"O outro agente disse: {texto_base}. Refine isso para o usu√°rio."}
        ]
    )
    return res_llama.choices[0].message.content

# --- 4. INTERFACE STREAMLIT ---
st.title("ü¶ô + ‚ú® Agentes em Cons√≠lio")

if "messages" not in st.session_state:
    st.session_state.messages = []

# Mostrar hist√≥rico
for m in st.session_state.messages:
    with st.chat_message(m["role"]):
        st.markdown(m["content"])

if prompt := st.chat_input("Mande sua d√∫vida..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.status("Consultando os especialistas...", expanded=True) as status:
            st.write("‚ú® Gemini est√° rascunhando...")
            # Aqui a m√°gica acontece sequencialmente
            resposta_final = fluxo_multi_agente(prompt)
            status.update(label="Revis√£o conclu√≠da pelo S√™nior √Åcido!", state="complete")
        
        st.markdown(resposta_final)
        st.session_state.messages.append({"role": "assistant", "content": resposta_final})
